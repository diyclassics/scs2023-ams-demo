{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLTK Readers demo notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A note on using Jupyter notebooks in this live demo*  \n",
    "\n",
    "- TLDR; \"SHIFT + ENTER/RETURN\" will do pretty much everything you will need to do for the demo.\n",
    "- Look for the word \"# Imports\" below and click anywhere is its surrounding box to activate that input cell. These cells are where you will find the code that we will run in today's demo.\n",
    "- If you press \"SHIFT + ENTER/RETURN\", you will *run* that cell and the output will appear in space below the cell; you will also advance to the next cell. If you press \"CTRL + ENTER/RETURN\", you will run the cell without advancing. If you press \"OPTION (or ALT) + ENTER/RETURN\", you will run the cell and insert a new empty cell below the running cell.\n",
    "- Note the brackets on the left side of the cells. If empty—that is, if there is blank space between the cells—the cell has not yet been run. If there is a number between the brackets, this is an indicator that the cell has been run and denotes the order in which the cells have been run."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A reader example—the LatinTesseraeCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "from cltkreaders.lat import LatinTesseraeCorpusReader\n",
    "\n",
    "from os.path import expanduser\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up reader\n",
    "# NB: If you do not have the CLTK-Tesserae corpus already installed in CLTK_DATA, you will be prompted to download the corpus.\n",
    "\n",
    "T = LatinTesseraeCorpusReader()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First 10 filesnames\n",
    "\n",
    "pprint(T.fileids()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First 10 works of Cicero\n",
    "\n",
    "cicero = [file for file in T.fileids() if 'cicero' in file]\n",
    "pprint(cicero[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Books of the Aeneid, sorted\n",
    "\n",
    "aeneid = natsorted([file for file in T.fileids() if 'aeneid' in file])\n",
    "pprint(aeneid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with doc structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catullus = 'catullus.carmina.tess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Docs\n",
    "\n",
    "catullus_doc = T.docs(catullus)\n",
    "print(next(catullus_doc)[:446])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Texts\n",
    "\n",
    "catullus_text = T.texts(catullus)\n",
    "print(next(catullus_text)[:335])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doc Rows\n",
    "\n",
    "catullus_docrows = T.doc_rows(catullus)\n",
    "\n",
    "print('This is a string representation of what the output dictionary looks like...')\n",
    "print(f'{str(next(catullus_docrows))[:94]} etc. }}\\n')\n",
    "\n",
    "\n",
    "catullus_docrows = T.doc_rows(catullus)\n",
    "print('Here are the first 10 items of the dict output...')\n",
    "pprint(list(next(catullus_docrows).items())[:10])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with doc units (i.e. philological units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catilinam = 'cicero.in_catilinam.tess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paras\n",
    "\n",
    "catilinam_paras = T.paras(catilinam)\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(f'Para {i}: {next(catilinam_paras)}')\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the Tesserae texts, `paras` are *not* truly implemented, as they are not consistent marked in the original files. For prose texts, what is returned by paras in the entire section by citation. The verse texts (which are defined by default as cited sections less than 75 characters long) yield the entire text as a single para, as for *Aeneid* 1 below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paras\n",
    "\n",
    "aeneid_paras = T.paras('vergil.aeneid.part.1.tess')\n",
    "\n",
    "for i in range(1,2):\n",
    "    print(f'Para {i}: {next(aeneid_paras)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sents\n",
    "\n",
    "# By default, segmentation, tokenization, and other tagging is done using the spaCy model 'la_dep_cltk_sm'\n",
    "\n",
    "catilinam_sents = T.sents(catilinam)\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(f'Sent {i}: {next(catilinam_sents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words\n",
    "\n",
    "# By default, segmentation, tokenization, and other tagging is done using the spaCy model 'la_dep_cltk_sm'\n",
    "\n",
    "catilinam_words = T.words(catilinam)\n",
    "\n",
    "for i in range(1,10):\n",
    "    print(f'Word {i}: {next(catilinam_words)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can pass a preprocessor to `words` \n",
    "\n",
    "def custom_preprocess(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "catilinam_words = T.words(catilinam, preprocess=custom_preprocess)\n",
    "\n",
    "for i in range(1,10):\n",
    "    print(f'Word {i}: {next(catilinam_words)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenized sents\n",
    "\n",
    "# i.e. Sents in the form of a list of tuples of the form `(token, lemma, tag)`\n",
    "\n",
    "catilinam_tokenized_sents = T.tokenized_sents(catilinam)\n",
    "\n",
    "for i in range(1,4):\n",
    "    print(f'Tok Sent {i}: {next(catilinam_tokenized_sents)}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenized sents, simplified\n",
    "\n",
    "# i.e. Sents in the form of a list of tokens\n",
    "\n",
    "catilinam_tokenized_sents = T.tokenized_sents(catilinam, simple=True)\n",
    "\n",
    "for i in range(1,4):\n",
    "    print(f'Tok Sent {i}: {next(catilinam_tokenized_sents)}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS-tagged sents\n",
    "\n",
    "# i.e. Sents in the form of a list of strings of the form `token/POS`\n",
    "\n",
    "catilinam_pos_sents = T.pos_sents(catilinam)\n",
    "\n",
    "for i in range(1,2):\n",
    "    print(f'POS Sent {i}: {next(catilinam_pos_sents)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with doc descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamorphoses = natsorted([file for file in T.fileids() if 'ovid.metamorphoses' in file])\n",
    "pprint(metamorphoses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_preprocess(text):\n",
    "    from cltk.alphabet.lat import JVReplacer\n",
    "    replacer = JVReplacer()\n",
    "\n",
    "    text = text.lower() # Lowercase\n",
    "    text = replacer.replace(text)  # Normalize u/v & i/j\n",
    "\n",
    "    # Remove punctuation\n",
    "    punctuation =\"\\\"#$%&\\'()*+,/:;<=>@[\\]^_`{|}~.?!«»—“-”\"\n",
    "    misc = '¡£¤¥¦§¨©¯°±²³´µ¶·¸¹º¼½¾¿÷·–‘’†•ↄ∞⏑〈〉（）'\n",
    "    misc += punctuation\n",
    "    translator = str.maketrans({key: \" \" for key in misc})\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # Remove numbers\n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    return \" \".join(text.split()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concordance, using Tesserae citations\n",
    "\n",
    "# NB: Concordancing is current only available for the Tesserae readers\n",
    "\n",
    "metamorphoses_concordances = T.concordance(metamorphoses, preprocess=custom_preprocess)\n",
    "\n",
    "met_conc_sample = next(metamorphoses_concordances)\n",
    "pprint(list(met_conc_sample.items())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordances are by default built on a file-by-file basis, but can easily be combined with the `compiled` parameter\n",
    "\n",
    "metamorphoses_concordances = T.concordance(metamorphoses, compiled=True, preprocess=custom_preprocess)\n",
    "\n",
    "full_met_conc_sample = next(metamorphoses_concordances)\n",
    "pprint(list(full_met_conc_sample.items())[96:102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the concordances are output as dictionaries, you can retrieve location information using the token as a dict key...\n",
    "\n",
    "metamorphoses_concordances = T.concordance(metamorphoses, compiled=True, preprocess=custom_preprocess)\n",
    "full_met_conc_sample = next(metamorphoses_concordances)\n",
    "\n",
    "print(f'\\'corpus\\' appears {len(full_met_conc_sample[\"corpus\"])} times in the Metamorphoses.')\n",
    "print('Here are the first five instances...')\n",
    "print(full_met_conc_sample['corpus'][:5])\n",
    "\n",
    "print()\n",
    "\n",
    "print(f'\\'corpora\\' appears {len(full_met_conc_sample[\"corpora\"])} times in the Metamorphoses.')\n",
    "print('Here are the first five instances...')\n",
    "print(full_met_conc_sample['corpora'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Basic descriptive data; note takes several minutes to run\n",
    "\n",
    "# tess_describe = T.describe()\n",
    "# pprint(tess_describe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output:  \n",
    "\n",
    "{'files': 748,  \n",
    " 'lexdiv': 24.255701516259066,  \n",
    " 'secs': 143.71532320976257,  \n",
    " 'sents': 314436,  \n",
    " 'vocab': 329693,  \n",
    " 'words': 7996935}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This data can also be returned for individual files or lists of files\n",
    "\n",
    "print('Stats on just the file \\'catullus.carmina.tess\\'')\n",
    "pprint(T.describe(catullus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Basic descriptive data; note takes several minutes to run\n",
    "\n",
    "# print('Stats on just the group of files assigned above to the variable `metamorphoses`')\n",
    "# pprint(T.describe(metamorphoses))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats on just the group of files assigned above to the variable `metamorphoses`  \n",
    "\n",
    "{'files': 15,  \n",
    " 'lexdiv': 4.389574250997125,  \n",
    " 'secs': 58.47328305244446,  \n",
    " 'sents': 6140,  \n",
    " 'vocab': 21562,  \n",
    " 'words': 94648}  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another reader example—the GreekTesseraeReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "from cltkreaders.grc import GreekTesseraeCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up reader\n",
    "# NB: If you do not have the CLTK-Tesserae corpus already installed in CLTK_DATA, you will be prompted to download the corpus.\n",
    "\n",
    "T = GreekTesseraeCorpusReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First 10 filesnames\n",
    "\n",
    "pprint(T.fileids()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apollonius = [file for file in T.fileids() if 'apollonius' in file]\n",
    "\n",
    "pprint(natsorted(apollonius))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sents\n",
    "\n",
    "# By default, segmentation, tokenization, and other tagging is done using the cltk v1 pipeline\n",
    "\n",
    "apollonius_sents = T.sents(apollonius)\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(f'Sent {i}: {next(apollonius_sents)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sents, with unline\n",
    "\n",
    "# By default, segmentation, tokenization, and other tagging is done using the cltk v1 pipeline\n",
    "\n",
    "apollonius_sents = T.sents(apollonius, unline=True)\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(f'Sent {i}: {next(apollonius_sents)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words\n",
    "\n",
    "# By default, segmentation, tokenization, and other tagging is done using the cltk v1 pipeline\n",
    "\n",
    "apollonius_words = T.words(apollonius, preprocess=custom_preprocess)\n",
    "\n",
    "for i in range(1,10):\n",
    "    print(f'Word {i}: {next(apollonius_words)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another reader example—the UDCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltkreaders.readers import UDCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up reader\n",
    "\n",
    "UD = UDCorpusReader('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print fileids\n",
    "\n",
    "print(UD.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw text; sample from Phaedrus 2 prologue\n",
    "\n",
    "print(UD.raw()[:1710])\n",
    "print('etc...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words\n",
    "\n",
    "# This reader reads also annotations directly from data files\n",
    "\n",
    "ud_words = UD.words()\n",
    "\n",
    "for i in range(1,10):\n",
    "    print(f'Word {i}: {next(ud_words)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words, with preprocessing\n",
    "\n",
    "# This reader reads also annotations directly from data files\n",
    "\n",
    "ud_words = UD.words(preprocess=custom_preprocess)\n",
    "\n",
    "for i in range(1,10):\n",
    "    print(f'Word {i}: {next(ud_words)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotated sents, directly from data files\n",
    "\n",
    "annotated_sents = UD.annotated_sents()\n",
    "\n",
    "for sent in annotated_sents:\n",
    "    pprint(sent)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conllu files transformed to Python dicts\n",
    "\n",
    "next(UD.sent_dicts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scs2023-ams-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d8c449db701cbfd673d73d3b2197958af987dac4c67dc9efb7e5d3b36d1867a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
